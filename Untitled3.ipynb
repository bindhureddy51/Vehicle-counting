{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1A932wbYPQfXkkN9N1_AdKhF5O3Yl7tH4",
      "authorship_tag": "ABX9TyOf6lqoNDBKYb7c8fKGscaL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bindhureddy51/Vehicle-counting/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y045NH-jJJED",
        "outputId": "133cb113-327e-40fb-bac6-6572aceeb67b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/extracted_folder/\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = '/content/drive/MyDrive/aichallenger.zip'\n",
        "\n",
        "# Specify the folder where you want to extract the files\n",
        "output_folder = '/content/extracted_folder/'\n",
        "\n",
        "# Make sure the output folder exists\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Open and extract the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_folder)\n",
        "\n",
        "print(f'Files extracted to {output_folder}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "# Feature Tokenizer Module: Combines CNN and Transformer\n",
        "class FeatureTokenizer(nn.Module):\n",
        "    def _init(self, input_channels=3, patch_size=16, embed_dim=768):  # Corrected __init_\n",
        "        super(FeatureTokenizer, self)._init_()\n",
        "        self.patch_size = patch_size\n",
        "        self.conv = nn.Conv2d(input_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, (224 // patch_size) ** 2, embed_dim))  # Positional embeddings\n",
        "        self.transformer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)  # Convolution to extract patches\n",
        "        x = x.flatten(2).transpose(1, 2)  # Flatten and reorder for Transformer\n",
        "        x = x + self.pos_embed  # Add positional embeddings\n",
        "        x = self.transformer(x)\n",
        "        return x\n",
        "\n",
        "# Token Encoder Module: Multi-head attention and MLP\n",
        "class TokenEncoder(nn.Module):\n",
        "    def _init(self, embed_dim=768):  # Corrected __init_\n",
        "        super(TokenEncoder, self)._init_()\n",
        "        self.mha = nn.MultiheadAttention(embed_dim, num_heads=8, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim * 4, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.mha(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        mlp_out = self.mlp(x)\n",
        "        x = self.norm2(x + mlp_out)\n",
        "        return x\n",
        "\n",
        "# Multi-Label Decoder Module with Residual Connections\n",
        "class MultiLabelDecoder(nn.Module):\n",
        "    def _init_(self, embed_dim=768, num_classes=61):  # Set for 61 classes\n",
        "        super(MultiLabelDecoder, self)._init_()\n",
        "        self.mha_self = nn.MultiheadAttention(embed_dim, num_heads=8, batch_first=True)\n",
        "        self.mha_cross = nn.MultiheadAttention(embed_dim, num_heads=8, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim * 4, embed_dim)\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        self_attn_out, _ = self.mha_self(x, x, x)\n",
        "        x = self.norm1(x + self_attn_out)\n",
        "        cross_attn_out, _ = self.mha_cross(x, context, context)\n",
        "        x = self.norm2(x + cross_attn_out)\n",
        "        x = self.mlp(x)\n",
        "        x = self.fc(x.mean(dim=1))  # Global average pooling\n",
        "        return x\n",
        "\n",
        "# Main LDI-NET Architecture\n",
        "class LDINet(nn.Module):\n",
        "    def _init_(self, input_channels=3, embed_dim=768, patch_size=16, num_classes=61):  # Adjusted for 61 classes\n",
        "        super(LDINet, self)._init_()\n",
        "        self.feature_tokenizer = FeatureTokenizer(input_channels, patch_size, embed_dim)\n",
        "        self.token_encoder = TokenEncoder(embed_dim)\n",
        "        self.multi_label_decoder = MultiLabelDecoder(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        tokens = self.feature_tokenizer(x)\n",
        "        encoded_tokens = self.token_encoder(tokens)\n",
        "        out = self.multi_label_decoder(encoded_tokens, tokens)\n",
        "        return out\n",
        "\n",
        "# Dataset Preparation\n",
        "def prepare_dataloader(data_path, batch_size=32):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    dataset = datasets.ImageFolder(data_path, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "# Training Function with Progress Bar and Logging\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, num_epochs=10):\n",
        "    criterion = nn.BCEWithLogitsLoss()  # For multi-label classification\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        loop = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        for i, (inputs, labels) in loop:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Convert labels to one-hot encoding\n",
        "            labels_one_hot = F.one_hot(labels, num_classes=61).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels_one_hot)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {running_loss / len(train_loader):.4f}\")\n",
        "        validate_model(model, val_loader, device)\n",
        "\n",
        "\n",
        "# Validation Function\n",
        "def validate_model(model, val_loader, device):\n",
        "    model.eval()\n",
        "    preds, labels_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds.append((outputs > 0.5).cpu())  # Threshold for multi-label\n",
        "            labels_list.append(labels.cpu())\n",
        "    preds = torch.cat(preds)\n",
        "    labels_list = torch.cat(labels_list)\n",
        "    accuracy = accuracy_score(labels_list.numpy(), preds.numpy())\n",
        "    print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Main Script\n",
        "if __name__ == \"_main_\":\n",
        "    # Paths and Device Configuration\n",
        "    train_data_path = \"/content/extracted_folder/aichallenger/train\"  # Update with the actual path\n",
        "    val_data_path = \"/content/extracted_folder/aichallenger/val\"      # Update with the actual path\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Prepare DataLoaders\n",
        "    train_loader = prepare_dataloader(train_data_path, batch_size=32)\n",
        "    val_loader = prepare_dataloader(val_data_path, batch_size=32)\n",
        "\n",
        "    # Model Initialization\n",
        "    model = LDINet(input_channels=3, embed_dim=768, patch_size=16, num_classes=61)  # 61 classes\n",
        "    model.to(device)\n",
        "\n",
        "    # Train and Validate\n",
        "    train_model(model, train_loader, val_loader, device, num_epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGZxs3FMRORk",
        "outputId": "1c0418c4-a9a3-47ac-e124-e1bd7df50c9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Feature Tokenizer Module: Combines CNN and Transformer\n",
        "class FeatureTokenizer(nn.Module):\n",
        "    def __init__(self, input_channels=3, patch_size=16, embed_dim=768):  # Corrected __init__\n",
        "        super(FeatureTokenizer, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.conv = nn.Conv2d(input_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, (224 // patch_size) ** 2, embed_dim))  # Positional embeddings\n",
        "        self.transformer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)  # Convolution to extract patches\n",
        "        x = x.flatten(2).transpose(1, 2)  # Flatten and reorder for Transformer\n",
        "        x = x + self.pos_embed  # Add positional embeddings\n",
        "        x = self.transformer(x)\n",
        "        return x\n",
        "\n",
        "# Token Encoder Module: Multi-head attention and MLP\n",
        "class TokenEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=768):  # Corrected __init__\n",
        "        super(TokenEncoder, self).__init__()\n",
        "        self.mha = nn.MultiheadAttention(embed_dim, num_heads=8, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim * 4, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.mha(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        mlp_out = self.mlp(x)\n",
        "        x = self.norm2(x + mlp_out)\n",
        "        return x\n",
        "\n",
        "# Multi-Label Decoder Module with Residual Connections\n",
        "class MultiLabelDecoder(nn.Module):\n",
        "    def __init__(self, embed_dim=768, num_classes=61):  # Set for 61 classes\n",
        "        super(MultiLabelDecoder, self).__init__()\n",
        "        self.mha_self = nn.MultiheadAttention(embed_dim, num_heads=8, batch_first=True)\n",
        "        self.mha_cross = nn.MultiheadAttention(embed_dim, num_heads=8, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim * 4, embed_dim)\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        self_attn_out, _ = self.mha_self(x, x, x)\n",
        "        x = self.norm1(x + self_attn_out)\n",
        "        cross_attn_out, _ = self.mha_cross(x, context, context)\n",
        "        x = self.norm2(x + cross_attn_out)\n",
        "        x = self.mlp(x)\n",
        "        x = self.fc(x.mean(dim=1))  # Global average pooling\n",
        "        return x\n",
        "\n",
        "# Main LDI-NET Architecture\n",
        "class LDINet(nn.Module):\n",
        "    def __init__(self, num_classes=61):  # Only passing num_classes for simplicity\n",
        "        super(LDINet, self).__init__()\n",
        "        self.feature_tokenizer = FeatureTokenizer(input_channels=3, patch_size=16, embed_dim=768)\n",
        "        self.token_encoder = TokenEncoder(embed_dim=768)\n",
        "        self.multi_label_decoder = MultiLabelDecoder(embed_dim=768, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        tokens = self.feature_tokenizer(x)\n",
        "        encoded_tokens = self.token_encoder(tokens)\n",
        "        out = self.multi_label_decoder(encoded_tokens, tokens)\n",
        "        return out\n",
        "\n",
        "# Dataset Preparation for Unlabeled Test Data\n",
        "class UnlabeledImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_folder, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, image_path\n",
        "\n",
        "def prepare_unlabeled_test_dataloader(data_path, batch_size=32):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    dataset = UnlabeledImageDataset(data_path, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    return dataloader, dataset\n",
        "\n",
        "# Generate Predictions\n",
        "def generate_predictions(model, test_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    image_paths = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, paths in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "            predictions.append(preds)\n",
        "            image_paths.extend(paths)\n",
        "    predictions = np.vstack(predictions)\n",
        "    return predictions, image_paths\n",
        "\n",
        "# Paths and Device Configuration\n",
        "test_data_path = \"/content/extracted_folder/aichallenger/testA\"  # Update with the actual path\n",
        "model_checkpoint_path = \"model_checkpoint.pth\"  # Path to the trained model checkpoint\n",
        "output_predictions_path = \"test_predictions.npy\"\n",
        "output_image_paths_file = \"test_image_paths.txt\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Prepare DataLoader\n",
        "test_loader, dataset = prepare_unlabeled_test_dataloader(test_data_path, batch_size=32)\n",
        "model_checkpoint_path = \"/content/model_checkpoint.pth\"\n",
        "torch.save(model.state_dict(), 'model_checkpoint.pth')\n",
        "\n",
        "import os\n",
        "\n",
        "# Correct model checkpoint path\n",
        "  # Ensure the correct path here\n",
        "\n",
        "# Check if the checkpoint file exists\n",
        "if not os.path.isfile(model_checkpoint_path):\n",
        "    raise FileNotFoundError(f\"Model checkpoint file not found at {model_checkpoint_path}\")\n",
        "\n",
        "# Load the Trained Model\n",
        "model = LDINet(num_classes=61)  # Adjusted for the correct number of classes\n",
        "model.load_state_dict(torch.load(model_checkpoint_path, weights_only=True))  # Use weights_only=True for safety\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Generate Predictions\n",
        "predictions, image_paths = generate_predictions(model, test_loader, device)\n",
        "\n",
        "# Save Predictions\n",
        "np.save(output_predictions_path, predictions)\n",
        "with open(output_image_paths_file, \"w\") as f:\n",
        "    f.writelines(\"\\n\".join(image_paths))\n",
        "\n",
        "print(f\"Predictions saved to {output_predictions_path}\")\n",
        "print(f\"Image paths saved to {output_image_paths_file}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHICwmmcS9V7",
        "outputId": "8e6c433f-cc37-449d-920c-cc3f83f4b8c8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to test_predictions.npy\n",
            "Image paths saved to test_image_paths.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "# Define your model checkpoint path\n",
        "model_checkpoint_path = \"/content/model_checkpoint.pth\"  # Adjust to your model's path\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transformation for a single image (same as for the batch data)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to match the model input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the single test image\n",
        "image_path = \"/content/extracted_folder/aichallenger/testA/00dd183fdc49db6f61af5a9de0e3d8ab.jpg\"  # Path to the single image you want to predict\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Apply the transformation to the image\n",
        "image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = LDINet(num_classes=61)  # Adjust the number of classes if needed\n",
        "model.load_state_dict(torch.load(model_checkpoint_path, weights_only=True))  # Load model weights safely\n",
        "model.to(device)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Generate Prediction for the Single Image\n",
        "with torch.no_grad():  # No need to track gradients for inference\n",
        "    outputs = model(image_tensor)  # Pass the image through the model\n",
        "    predictions = torch.sigmoid(outputs).cpu().numpy()  # Apply sigmoid for multi-label classification\n",
        "    predicted_class = np.argmax(predictions, axis=1)  # Get the class with the highest probability\n",
        "\n",
        "# Print the prediction results\n",
        "print(f\"Predicted class index: {predicted_class[0]}\")\n",
        "print(f\"Prediction probabilities: {predictions}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T8WD_9Janqu",
        "outputId": "38711bee-98a4-41ba-de00-c7f0cffd369e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class index: 59\n",
            "Prediction probabilities: [[0.51108456 0.5134737  0.5041784  0.4948417  0.50098866 0.50641245\n",
            "  0.49790868 0.48643762 0.48276508 0.4984841  0.48687145 0.5077355\n",
            "  0.47892025 0.5126781  0.48684838 0.50813264 0.5232973  0.49734095\n",
            "  0.50253594 0.4988868  0.5016016  0.49682796 0.4922758  0.4826482\n",
            "  0.4691294  0.5176645  0.5154398  0.5244732  0.48125678 0.50134826\n",
            "  0.50305647 0.50175256 0.48737222 0.51005673 0.478794   0.49576375\n",
            "  0.4785461  0.50519496 0.47160023 0.49935567 0.49944606 0.50430024\n",
            "  0.4800299  0.5046772  0.4882057  0.48407388 0.47247103 0.49540246\n",
            "  0.49930015 0.5215208  0.4758738  0.49935925 0.501565   0.51034516\n",
            "  0.52055764 0.50938773 0.5131076  0.4993666  0.49814805 0.53190243\n",
            "  0.5114557 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer  # For multi-label encoding\n",
        "\n",
        "# Adjusted Dataset class for validation with folder structure\n",
        "class ValidatedImageDataset(Dataset):\n",
        "    def __init__(self, image_folder, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.classes = os.listdir(image_folder)\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "\n",
        "        for cls in self.classes:\n",
        "            class_folder = os.path.join(image_folder, cls)\n",
        "            for image_name in os.listdir(class_folder):\n",
        "                image_path = os.path.join(class_folder, image_name)\n",
        "                self.image_paths.append(image_path)\n",
        "                # Assigning the label for each image (binary vector for multi-label classification)\n",
        "                label = [0] * len(self.classes)\n",
        "                label[self.class_to_idx[cls]] = 1\n",
        "                self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, np.array(label), image_path  # Return image, label, and path\n",
        "\n",
        "# Define the image transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Prepare validation dataset and dataloader\n",
        "validation_data_path = '/content/extracted_folder/aichallenger/val'  # Update to the actual validation data path\n",
        "validation_dataset = ValidatedImageDataset(image_folder=validation_data_path, transform=transform)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Model evaluation and accuracy calculation\n",
        "def calculate_accuracy(predictions, ground_truth_labels):\n",
        "    predicted_labels = (predictions > 0.5).astype(int)  # Threshold at 0.5 for multi-label classification\n",
        "    correct_predictions = (predicted_labels == ground_truth_labels).astype(int)\n",
        "    accuracy = correct_predictions.mean()  # Mean of correct predictions\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "predictions = []\n",
        "ground_truth_labels = []\n",
        "image_paths = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels, paths in validation_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)  # Ensure labels are on the same device\n",
        "        outputs = model(inputs)\n",
        "        preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "\n",
        "        predictions.append(preds)\n",
        "        ground_truth_labels.append(labels.cpu().numpy())  # Store true labels\n",
        "        image_paths.extend(paths)\n",
        "\n",
        "# Convert predictions and labels to numpy arrays for accuracy calculation\n",
        "predictions = np.vstack(predictions)\n",
        "ground_truth_labels = np.vstack(ground_truth_labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = calculate_accuracy(predictions, ground_truth_labels)\n",
        "print(f\"Validation Multi-label Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QUlBA7dbCd-",
        "outputId": "f260b1ad-19da-4b0d-f0f7-8250ddc33e46"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Multi-label Accuracy: 0.5346\n"
          ]
        }
      ]
    }
  ]
}